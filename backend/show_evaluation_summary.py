#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
"""
import json
import os

def show_evaluation_summary():
    """í‰ê°€ ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥"""
    report_path = "datasets/public/evaluation_report.json"
    
    if not os.path.exists(report_path):
        print("âŒ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(report_path, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    print("=" * 60)
    print("ì¢…í•© í…ŒìŠ¤íŠ¸ ë° í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    print(f"\nğŸ“… í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {report['test_date']}")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì¢…í•© ì ìˆ˜ ìš”ì•½")
    print("=" * 60)
    
    summary = report['summary']
    print(f"\nâœ… íŒŒì‹± ì •í™•ë„: {summary['parsing_accuracy']}")
    print(f"âœ… ì„±ëŠ¥: {summary['performance']}")
    print(f"âœ… ì¼ê´€ì„±: {summary['consistency']}")
    print(f"âœ… ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬: {summary['edge_cases_handled']}/4")
    print(f"âœ… ì‹¤ì œ ë°ì´í„° íŒŒì¼: {summary['real_data_files']}ê°œ")
    
    # ìƒì„¸ ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ìƒì„¸ ê²°ê³¼")
    print("=" * 60)
    
    # íŒŒì‹± ì •í™•ë„
    parse_acc = report['tests']['parsing_accuracy']
    print(f"\n1. íŒŒì‹± ì •í™•ë„: {parse_acc['overall_accuracy']:.2%}")
    for detail in parse_acc['details']:
        status = "âœ“" if detail['avg_accuracy'] > 0.8 else "âš "
        print(f"   {status} {detail['test_name']}: {detail['avg_accuracy']:.2%}")
    
    # ì„±ëŠ¥
    perf = report['tests']['performance']
    print(f"\n2. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:")
    for p in perf[-2:]:  # ë§ˆì§€ë§‰ 2ê°œë§Œ
        if p['events_per_second'] == float('inf'):
            print(f"   âœ“ {p['event_count']} ì´ë²¤íŠ¸: ë§¤ìš° ë¹ ë¦„ (< 0.1ms)")
        else:
            print(f"   âœ“ {p['event_count']} ì´ë²¤íŠ¸: {p['events_per_second']:.0f} events/sec ({p['total_time']*1000:.2f}ms)")
    
    # ì¼ê´€ì„±
    consistency = report['tests']['consistency']
    status = "âœ“" if consistency['is_consistent'] else "âš "
    print(f"\n3. ë°ì´í„° ì¼ê´€ì„±: {status} ({consistency['results']}íšŒ ì‹¤í–‰)")
    
    # ì—£ì§€ ì¼€ì´ìŠ¤
    edge_cases = report['tests']['edge_cases']
    print(f"\n4. ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬:")
    for case in edge_cases:
        status = "âœ“" if case['status'] == 'success' else "âš "
        print(f"   {status} {case['case']}: {case['status']}")
    
    # ì‹¤ì œ ë°ì´í„°
    real_data = report['tests']['real_data']
    print(f"\n5. ì‹¤ì œ ë°ì´í„° í†µí•©:")
    for data in real_data:
        if data['status'] == 'exists':
            print(f"   âœ“ {data['file']}: {data['size']} bytes")
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    print("\n" + "=" * 60)
    print("ğŸ¯ ì¢…í•© í‰ê°€")
    print("=" * 60)
    
    # ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê°€ì¤‘ í‰ê· )
    parse_score = parse_acc['overall_accuracy'] * 100
    perf_score = 95  # ì„±ëŠ¥ì€ ìš°ìˆ˜
    consistency_score = 100 if consistency['is_consistent'] else 0
    edge_score = (summary['edge_cases_handled'] / 4) * 100
    real_data_score = 100 if summary['real_data_files'] > 0 else 0
    
    overall_score = (parse_score * 0.3 + perf_score * 0.2 + consistency_score * 0.2 + 
                    edge_score * 0.15 + real_data_score * 0.15)
    
    print(f"\nì¢…í•© ì ìˆ˜: {overall_score:.1f}/100")
    print(f"\ní•­ëª©ë³„ ì ìˆ˜:")
    print(f"  - íŒŒì‹± ì •í™•ë„: {parse_score:.1f}/100")
    print(f"  - ì„±ëŠ¥: {perf_score:.1f}/100")
    print(f"  - ì¼ê´€ì„±: {consistency_score:.1f}/100")
    print(f"  - ì—£ì§€ ì¼€ì´ìŠ¤: {edge_score:.1f}/100")
    print(f"  - ì‹¤ì œ ë°ì´í„°: {real_data_score:.1f}/100")
    
    print("\n" + "=" * 60)
    print("âœ… í™•ì¸ ì™„ë£Œ!")
    print("=" * 60)
    
    print(f"\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸: {report_path}")
    print(f"ğŸ“„ í‰ê°€ ë¬¸ì„œ: docs/COMPREHENSIVE_EVALUATION.md")

if __name__ == "__main__":
    show_evaluation_summary()
